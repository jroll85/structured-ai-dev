02\_code\_quality\_maintainability\_ai\_rules.md

\# LiveSales.us AI-Assisted Development: Code Quality & Maintainability

To keep our codebase healthy, AI-generated code must conform to our project’s coding standards and quality practices – just as if a team member wrote it. Cursor should be guided to produce code that is simple, clean, well-structured, and testable. This means adhering to principles like KISS (Keep It Simple, Stupid) and DRY (Don’t Repeat Yourself), using our established patterns (modular services, hooks, etc.), and avoiding “clever” but convoluted solutions. If an AI suggestion is too complex or “magical,” it’s often better to simplify it even if that means writing a bit more code. Always prioritize readability and maintainability over one-liner tricks.

\#\# Aligning with Coding Standards

Our project enforces strict TypeScript types, specific naming conventions, and file organization. When using Cursor, include these expectations in your prompt or use project rules so it knows our style. For example, if the project prefers functional programming patterns and early returns for error handling, the Cursor rules (in our \`.cursor/rules/\` directory) explicitly state that – \*and AI output should reflect it\*. We have rules requiring strong typing (no \`any\` unless absolutely necessary), so if Cursor provides an \`any\` or \`@ts-ignore\`, that’s a red flag – you should refine the prompt or manually adjust to eliminate loose types. Similarly, our convention might be to use \`camelCase\` for variables and \`PascalCase\` for components; ensure AI suggestions follow this. If it introduces a differently styled function or an inconsistent file structure, refactor or ask it to conform (“Refactor the code to match our naming conventions and folder structure”). Cursor’s ability to “chat with codebase” is useful here: you can point it to an existing module as an example of style. For instance: \*“Follow the pattern in \`services/orderService.ts\` for error handling and logging.”\* By giving context, you help the AI produce consistent code.

\#\# Modular, DRY Code

One risk with AI is that it might not recognize when a similar function already exists, leading to duplicate code. Always check if Cursor’s output repeats functionality we have. Use the project search (or ask Cursor) to see if a helper or component already exists before accepting a new one. We encourage using AI for refactoring to eliminate duplication: e.g. \*“These two functions look similar – AI, help me refactor into one generic utility.”\* The AI can assist in extracting common code, but verify it doesn’t break anything. Keep functions and components focused (single responsibility principle) – if Cursor generates a huge function that does too many things, consider prompting it to split logic into smaller functions or React components.

Error handling is another quality aspect: ensure AI-generated functions properly handle errors and edge cases (e.g. try/catch around async calls, graceful fallback UI for component errors, etc.), as per our standards. We prefer explicit error returns or exceptions with clear messages; if the AI omitted error handling, ask it to add it. All new code should also have corresponding unit tests or integration tests when appropriate – you can use Cursor to draft these tests, but make sure they cover happy path and edge cases.

\#\# AI-Assisted Refactoring

When using Cursor to improve existing code, do it in controlled steps. Our best practice: write or update tests before refactoring, then let AI suggest changes, and finally run tests to confirm nothing broke. Cursor’s “Fix & Diff” feature is great for this – it can apply changes and show a diff. Always review diffs line-by-line to ensure the AI didn’t alter something unintentionally. For example, if refactoring a data processing function, check that all branches from the original are still present. Do not allow the AI to make broad project-wide changes in one go (like renaming a widely used API) without incremental review. Use branch deployments and CI checks as usual; AI does not get a free pass on CI. Keep commit messages clear (you can even have Cursor draft a commit message, but verify its accuracy). If Cursor refactors code, have it also update any related comments or docs.

Finally, maintainability includes documentation (detailed in section 6 of "Cursor rules.md" (Source 10)) and keeping the codebase tidy. Cursor can help find unused code or suggest better structure, but always validate such suggestions with the team. For instance, if it says “Module X is not used,” double-check usage before deletion.

\#\# Code Quality Do’s and Don’ts:

\* \*\*Do\*\* configure Cursor’s project rules to embed our style guides (formatting, naming, architecture). For example, rules might specify: “Always use Shadcn UI components for UI elements, do not use plain \`\<input\>\` for styled input – use \`\<Input\>\` from our library.” The AI will then follow these by default. Keep the \`.cursorrules\` updated as our conventions evolve.  
\* \*\*Do\*\* use clear, specific prompts to guide the AI’s coding style. Mention the frameworks and libraries it should use (“use Next.js 14 App Router conventions”, “use Supabase client for DB calls”, “use Tailwind CSS for styling with mobile-first classes”). The more context, the better the output will align with our expectations.  
\* \*\*Do\*\* enforce strict type safety in AI output. All functions should have proper TypeScript types. If Cursor returns something partially typed or uses \`any\`, ask it to fix types or do it manually. Our codebase runs in strict mode – AI code must not introduce compile-time errors or type loopholes.  
\* \*\*Do\*\* generate unit tests or integration tests alongside new code. Use Cursor to create tests (e.g., “Write a Jest unit test for the above function covering edge cases”). This not only validates the AI’s code but also improves quality. The AI can often suggest tests you might not think of. Always run these tests yourself to ensure they pass and truly cover the logic.  
\* \*\*Do\*\* keep functions and components small and focused. If AI writes a 300-line function, consider prompting it to refactor into smaller pieces or do it yourself. It’s easier to maintain bite-sized code. Similarly, ensure AI uses our modular service pattern – e.g. data fetching in service files, presentation in components – rather than mixing concerns in one place.  
\* \*\*Do\*\* use AI for documentation and clarity (as detailed later) – well-documented code is maintainable code.  
\* \*\*Don’t\*\* accept code that violates our established standards. For example, if our rule is “all API responses must use our standardized response format,” don’t let AI introduce a custom JSON structure. Consistency is key to maintainability.  
\* \*\*Don’t\*\* allow duplicate logic. Before implementing an AI-suggested solution, search the codebase. The AI might not know we already have a \`formatDate()\` utility and could write a new one – avoid that by reusing existing code. If duplicates arise, consolidate them (AI can help merge two functions into one).  
\* \*\*Don’t\*\* over-engineer because the AI can. Sometimes AI will output an overly complex design pattern or a generic abstraction that we don’t need. If a simpler solution works, use it. We value clarity – so do not introduce design patterns (factories, singletons, etc.) unless there’s a known need, even if the AI eagerly suggests them.  
\* \*\*Don’t\*\* ignore warnings or lint. Our ESLint/Prettier and type checker might complain about AI code – fix those issues, don’t just disable the rules. If AI introduces a long function with too many branches (violating complexity rules), refactor it rather than suppressing the warning.  
\* \*\*Don’t\*\* use the AI to make wholesale refactors without a plan. Large-scale changes (like renaming a widely used model or migrating a library) should be broken into steps and tested at each step. Don’t let the AI modify dozens of files automatically unless each change is understood and verified. In Cursor, avoid using “Apply All Fixes” blindly; use the diff view and apply changes in stages, running tests frequently.

