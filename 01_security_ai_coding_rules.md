01\_security\_ai\_coding\_rules.md

\# LiveSales.us AI-Assisted Development: Security Best Practices with AI Code Generation

AI can accelerate development, but security must never be compromised in the process. Studies show that 30–50% of AI-generated code may contain vulnerabilities. Cursor (and any code assistant) is trained to produce functional code, not to instinctively enforce our security requirements. Developers must actively guide the AI and rigorously review its output to prevent common threats like cross-site scripting (XSS), SQL injection, and other exploits. Always remember that we are accountable for security, not the AI.

\#\# Mitigating Vulnerabilities

Be explicit in prompts about security controls. For example, if asking Cursor to build a form or API endpoint, instruct it to validate and sanitize all user inputs and use safe libraries or ORM parameterization. Verify that the output uses prepared statements or query builders for database access, encodes HTML to prevent XSS, and avoids dangerous functions or eval. \*Never\* assume the AI “got it right” on security – always double-check. If Cursor suggests code that directly concatenates user input into SQL or HTML, treat it as a bug. (E.g., \*“Ensure to use parameterized queries instead of string concatenation for SQL”\*). Remember that AI-suggested code may look correct at first glance but harbor subtle issues; a classic example is an AI-generated login that omits password hashing – it “works” but is grossly insecure.

Use automated scanners to catch issues: Cursor’s integration with Aikido Security can flag vulnerabilities (like injection points) in real time as code is generated, and suggest one-click fixes. We have enabled this and similar security linting plugins (e.g. SonarQube IDE extension) in our development environment for continuous SAST analysis within Cursor.

\#\# Handling Secrets and Sensitive Data

Never expose secrets (API keys, credentials, personal data) in your prompts or AI-visible code. Anything sent to the AI’s cloud could be retained or leak. Cursor operates in a cloud-assisted mode, so treat it as an external service for this purpose. Always use environment variables or secret management solutions in code – \*do not ask the AI to “hardcode” any secret\*. For example, an AI might naively insert a database password in plaintext if you don’t stop it. This is strictly forbidden. Instead, prompt Cursor to integrate with our secret vault (Supabase config or an AWS Secrets Manager) or use \`process.env\` variables, and ensure the AI’s output \*never\* prints or logs those values.

We enforce Privacy Mode in Cursor for all team members, which guarantees that code context is not stored or used to train models. Privacy Mode adds a layer of protection, but it’s not foolproof – so developers must still sanitize prompts by removing or anonymizing any user PII or confidential logic before sending it to Cursor. Use fake examples (e.g. dummy emails, keys like XXXX) if needed to illustrate an issue to the AI.

By policy, all secrets in our repo (API keys, credentials) must reside in our \`.env\` files or secret manager; if you see Cursor outputting a secret, revoke it immediately and replace with env usage. We also leverage secret-scanning tools (like a pre-commit Git hook and Aikido’s scanner) to catch any accidental secret in code.

\#\# Safe Dependencies and Packages

When Cursor suggests installing a new library or copying code from a third-party source, vet it just as you would any dependency added manually. AI can sometimes hallucinate imports or recommend obscure packages that might be unmaintained or even malicious. Before running \`npm install\` on an AI-suggested package, verify the package name and check its NPM reputation (downloads, maintenance), license, and whether it fits our approved tech stack. For example, if Cursor suggests using a random image resizing library, ensure we don’t already have a standard solution and confirm the library has no known vulnerabilities. Do not blindly trust AI on security-critical code from external sources. Treat such suggestions as potential supply chain risks: inspect the code or source repo if possible. We maintain an allowlist of approved libraries; any new addition requires human approval via code review. Also be cautious if Cursor generates code that pulls in a CDN script or external API – confirm that it’s a safe, trusted source. Always prefer our existing solutions (for instance, our established Supabase client for database queries, or Shadcn/UI components for UI) over novel AI-suggested alternatives. If unsure, consult the team or a senior dev before using a new library the AI introduced.

\#\# Secure Coding Do’s and Don’ts:

\* \*\*Do\*\* explicitly instruct Cursor to implement security best practices (validation, sanitation, auth checks) in every relevant prompt. For example: \*“Generate a Next.js API route to fetch user data, and ensure it checks the user’s JWT and uses parameterized queries.”\*  
\* \*\*Do\*\* use Cursor’s Privacy Mode and organization settings to prevent code data from leaving our control. Double-check that Privacy Mode is enabled on your Cursor client (it’s enforced for our team).  
\* \*\*Do\*\* leverage security plugins: e.g. Aikido for Cursor to catch vulns as you code, and fix suggestions, or the SonarQube Cursor extension for static analysis in-editor. These tools provide immediate feedback on AI-written code (flagging things like XSS, SQLi, insecure dependencies).  
\* \*\*Do\*\* keep sensitive info out of prompts and code examples. Mask real emails, names, or IDs when asking Cursor for help with data-specific logic (use “Alice (test user)” or sample data instead of actual customer data).  
\* \*\*Do\*\* validate AI outputs with tests and reviews (SQL queries from AI should be tested against injection; UI rendering should be tested for XSS by simulating malicious input). Treat AI code as if written by a junior developer – \*trust but verify\* every security-relevant line.  
\* \*\*Don’t\*\* allow Cursor to introduce known vulnerability patterns. If you see the AI produce raw HTML insertion (\`dangerouslySetInnerHTML\`) or dynamic SQL/NoSQL queries string-building from input, stop and correct it. Don’t accept code that disables ESLint security rules or TypeScript strict checks either – those rules exist to protect us.  
\* \*\*Don’t\*\* include actual passwords, API keys, access tokens, or user personal data in any prompt to Cursor. This violates GDPR/CCPA data minimization and could leak secrets externally. Instead, if you need AI help with an algorithm that uses a key, use a placeholder and explain (“use an API key stored in env variable”).  
\* \*\*Don’t\*\* execute AI-generated code blindly. Always run it in a safe environment first. For example, if Cursor writes a database migration or file operation, review it carefully; never run an AI-suggested shell command like \`rm \-rf\` or destructive SQL without understanding exactly what it does. In fact, we maintain a blocklist of destructive commands that should never be run – e.g. dropping tables, mass deletes – Cursor should not suggest these, but if it does, \*do not run them\*.  
\* \*\*Don’t\*\* let the AI be the final gate for security. Human oversight is required (as detailed in section 5 of "Cursor rules.md" (Source 10)). No code (especially security-sensitive code) goes to production without a human code review, no matter how confident the AI sounds.  
